# ============================================================================
# 2013-04-07 (Sunday) 00:58 IST
#
# Get beta click database and start writing analysis code for it.

cd analysis/workspace

wget http://spacewarps.org.s3.amazonaws.com/data-export/spacewarps-2013-04-06_20-07-28.tar.gz

tar xvfz spacewarps-2013-04-06_20-07-28.tar.gz

ls spacewarps-2013-04-06_20-07-28/ouroboros_staging/
# spacewarp_classifications.bson          spacewarp_subjects.bson
# spacewarp_classifications.metadata.json spacewarp_subjects.metadata.json

# These are files to be read into a mongoDB, using mongodbrestore.

# Needs pymongo... pip install? Yep - done

# Also need binary installation of mongodb, from mongodb.org
# Downloaded to software, linked to ~/bin

mongorestore spacewarps-2013-04-06_20-07-28
# Sun Apr  7 01:23:48.133 kern.sched unavailable
# couldn't connect to [127.0.0.1] couldn't connect to server 127.0.0.1:27017

# Hmm. Looks beyond my pay grade.
# Try from python:

cd spacewarps-2013-04-06_20-07-28/
python ../../swap/mongodb.py

Traceback (most recent call last):
  File "../../swap/mongodb.py", line 23, in <module>
    m = MongoDB()
  File "../../swap/mongodb.py", line 13, in __init__
    self.client = MongoClient('localhost', 27017)
  File "/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pymongo/mongo_client.py", line 336, in __init__
    raise ConnectionFailure(str(e))
pymongo.errors.ConnectionFailure: could not connect to localhost:27017: [Errno 61] Connection refused

# Need to start mongodb server in the background:

mongod --dbpath . &

# Now python runs ok:

python ../../swap/mongodb.py
# Collection(Database(MongoClient('localhost', 27017), u'ouroboros_staging'), u'spacewarp_subjects.0')
# Collection(Database(MongoClient('localhost', 27017), u'ouroboros_staging'), u'spacewarp_classifications.0')

# What are these things?!
# OK, open up python session and start playing.

>>> import swap
>>> m = swap.MongoDB()
>>> m.subjects
Collection(Database(MongoClient('localhost', 27017), u'ouroboros_staging'), u'spacewarp_subjects')
>>> m.subjects.find()
<pymongo.cursor.Cursor object at 0x101220f50>

# Interesting.

>>> m.subjects.find_one({'zooniverse_id' : 'ASW0000002'})
{u'project_id': ObjectId('5101a1341a320ea77f000001'),
u'classification_count': 523, u'created_at': datetime.datetime(2013, 4,
4, 20, 29, 18, 949000), u'random': 0.5465496445414518, u'updated_at':
datetime.datetime(2013, 4, 4, 20, 29, 18, 975000), u'state':
u'complete', u'trending': 3, u'coords': [], u'location': {u'thumbnail':
u'', u'standard': u''}, u'zooniverse_id': u'ASW0000002',
u'workflow_ids': [ObjectId('5101a1361a320ea77f000002')], u'_id':
ObjectId('5101a1931a320ea77f000004'), u'tutorial': True, u'metadata':
{}}

# OK cool - here's a clue to the subject schema, and the metadata on subject
# ASW0000002.

# How about a non-tutorial one?
>>> m.subjects.find_one({'zooniverse_id' : 'ASW0000004'}) {u'_id':
ObjectId('515de29fe4bb216427000001'), u'classification_count': 13,
u'created_at': datetime.datetime(2013, 4, 4, 20, 29, 19, 372000),
u'activated_at': datetime.datetime(2013, 4, 4, 20, 32, 51, 886000),
u'updated_at': datetime.datetime(2013, 4, 4, 20, 29, 19, 398000),
u'random': 0.1047423015104475, u'project_id':
ObjectId('5101a1341a320ea77f000001'), u'state': u'active',
u'zooniverse_id': u'ASW0000004', u'workflow_ids':
[ObjectId('5101a1361a320ea77f000002')], u'location': {u'thumbnail':
u'http://www.spacewarps.org/subjects/thumbnail/CFHTLS_002_0530_gri.png',
u'standard':
u'http://www.spacewarps.org/subjects/standard/CFHTLS_002_0530_gri.png'},
u'group_id': ObjectId('5154a3783ae74086ab000001'), u'metadata': {u'id':
u'CFHTLS_002_0530'}}

# Nice! There's the actual image name, from Anu. And this URL is open -
# volunteers can grab those any time if they want to model them elsewhere.
# Just need to expose that on the site somewhere.

# Anyway, what about classifications?

>>> m.classifications.find_one()
{u'_id': ObjectId('515de3a9390c056085000416'), u'created_at':
datetime.datetime(2013, 4, 4, 20, 33, 45), u'updated_at':
datetime.datetime(2013, 4, 4, 20, 33, 45, 885000), u'user_ip':
u'163.1.174.106', u'workflow_id': ObjectId('5101a1361a320ea77f000002'),
u'subjects': [], u'subject_ids': [ObjectId('515dd45de4bb21597c00026c')],
u'project_id': ObjectId('5101a1341a320ea77f000001'), u'annotations':
[{u'finished_at': u'Thu, 04 Apr 2013 20:32:19 GMT', u'started_at': u'Thu, 04
Apr 2013 19:57:45 GMT'}, {u'user_agent': u'Mozilla/5.0 (Macintosh; Intel Mac
OS X 10_7_5) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.43
Safari/537.31'}]}

>>> m.classifications.find({'_id':'515de3a9390c056085000416'})
# <pymongo.cursor.Cursor object at 0x10122c050>

# Hmm. A cursor seems to be some sort of cursor. Might need a bit of help
# learning how this works!

# Plan:
#
# Make a list of operations that I want to do,
# and ask for a suggested example command from Amit?
#
# 1) Return a list of users as an array of strings
#
# 2) For a given user, return the IDs of all the training subjects they classified
#     Presumably this array will be time-ordered?
#
# 3) For a given user and training subject ID, return the value 1 if they succeeded in recognising it, 0 if not

# Issued.


# 2013-04-18 (Thursday) 12:04 BST
#
# datetime objects are interesting! See http://docs.python.org/2/library/datetime.html
#
# >>> import datetime
# >>> print datetime.datetime(2013, 4, 4, 20, 29, 18, 949000)
# 2013-04-04 20:29:18.949000
# >>> t1 = datetime.datetime(2013, 4, 4, 20, 29, 18, 949000)
# >>> t2 = datetime.datetime(2013, 4, 5, 8, 15, 23, 543000)
# >>> t2 - t1
# datetime.timedelta(0, 42364, 594000)
# >>> t1 > t2
# False
# >>> t1 <= t2
# True

# Perfect!

# ============================================================================
# 2013-04-22 (Monday) 23:24 BST

# Getting data out of mongodb is not so intuitive. Wrapped it up in
# python so that I can just do, eg

cd workspace
python ../swap/mongodb.py

# But I cannot do even the most basic things - so I've written down what
# I want to do in plausible python, for Amit to advise on!

# ============================================================================
# 2013-04-24 (Wednesday) 10:18 BST
#
# Great session with Amit yesterday solving all my db access problems!
# He says that on the Zooniverse system, the correct db is always online,
# so we should just connect to the client. That means that all of this
# wrapper I wrote is redundant - but I preserve it here, for reference:

#     self.dumpname = dumpname
#
#     # Keep a record of what goes on:
#     self.logfilename = os.getcwd()+'/'+self.dumpname+'.log'
#     self.logfile = open(self.logfilename,"w")
#
#     # Start the Mongo server in the background:
#     subprocess.call(["mongorestore",self.dumpname],stdout=self.logfile,stderr=self.logfile)
#     os.chdir(self.dumpname)
#     self.cleanup()
#     self.process = subprocess.Popen(["mongod","--dbpath","."],stdout=self.logfile,stderr=self.logfile)
#
#     # Check everything is working:
#     if self.process.poll() == None: print "MongoDB: server is up and running"
#
#     # Connect to the Mongo:
#     self.client = MongoClient('localhost', 27017)
#     self.db = self.client['ouroboros_staging']
#
#     self.subjects = self.db['spacewarp_subjects']
#     self.classifications = self.db['spacewarp_classifications']

# Followed by, later,
#     m.terminate()

# Worked example gives the following result, from the initial beta DB:

# Counted  7564  classifications, that each look like:
# ('2013-04-06 19:54:42.146000', '5065ff08d10d244cd10029ba', '515de2efe4bb21642700019a', 'test', 'NOT')

# ============================================================================
# 2013-04-29 (Monday) 17:03 BST

# OK, got SWAP working in its simplest incarnation.
# REady to do some experiments!

# Standard setup is this on: agents learn, but set PD=PL=50%, initially.
SWAP.py CFHTLS-beta_P50.config

# Aprajita asks, do we have to learn? Why not assume one set of P's and
# stick with them? Get off to a fsater start?
SWAP.py CFHTLS-beta_no-learning_P50.config
# Random classifiers get nowhere!

SWAP.py CFHTLS-beta_no-learning_P90.config
# Big jumps up and down in probability, lots of false positives.


# OK, so back to learning. How about we be less pessimistic about people's
# talents?
SWAP.py CFHTLS-beta_P90.config
# Not bad - Christmas tree is broader. FP rate higher, but got some sims in
# the detection zone now.

# More measured. Crowd divided between two groups?
#    The Herd: PD = 0.9, PL = U(0:1)
# Enthusiasts: PD = U(0.2:0.6), PL = U(0.6:1.0)
SWAP.py CFHTLS-beta_P70.config


# One issue with this LENS or NOT analysis is the hard edged estimation of PD
# and PL. Early classifications can have a significant impact... Hmm. 70-70
# seems like a reasonable starting point.


# Wishlist for mock survey:

# - Difficulty variation              DONE
# - PD, PL capped at 0.99             DONE
# - Realistic PDFs for Nc, PD, PL     DONE

# ============================================================================
# 2013-05-06 (Monday) 12:40 BST

# Wishlist for mock survey (continued):

# - Completeness, purity calculated   DONE 2013-05-06
# - Detected/rejected subjects        DONE 2013-05-06
# - Retired/promoted subjects         DONE 2013-05-06
# - Candidate IDs output              DONE 2013-05-06

# OK, great! Now we just need to be able to do low cost
# batch processing.


# ============================================================================
# 2013-05-07 (Tuesday) 09:43 BST

# Testing new database!

# Download:

set url = "https://zooniverse-code.s3.amazonaws.com/databases/2013-05-07/ouroboros_projects/spacewarp_2013-05-07.tar.gz?AWSAccessKeyId=AKIAJHHZ7KLFECQKTS7A&Expires=1368548396&Signature=as2nCaTgD9ctFLIHZrxj%2FrnQJoo%3D"

set dbfile = `echo "$url" | cut -d '?' -f1 | cut -d'/' -f7`
set logfile = ${dbfile:r:r}.wget.log
wget -O $dbfile "$url" >& $logfile

# Unpack:
tar xvfz $dbfile

set db = $dbfile:r:r

# First need to kill any old servers:
set pid = `ps -e | grep 'mongod --dbpath' | \
                   grep -v 'grep' | head -1 | awk '{print $1}'`
if ($#pid > 0) kill $pid

# Start new mongo server in its own directory, out of the way:
mkdir -p mongo
chdir mongo
mongod --dbpath . &
chdir ..

# Now restore the new database:
mongorestore --drop --db ouroboros_staging $db

# Probably should script this? Maybe?


# Try running SWAP:

SWAP.py CFHTLS-test.config >& CFHTLS-test.log

# SWAP: interpreting classifications...
# SWAP: total no. of classifications processed:  0


# Scripted! Just need to download db tarball manually:

SWIPE.csh spacewarp_2013-05-07.tar.gz


# ============================================================================
# 2013-05-07 (Tuesday) 18:36 BST

# Control of retirement from afar! Coooooool.

# Email from Michael Parrish:


# Hi Phil,
# 
# I'm attaching an example administration client script (in python) along
# with the documentation for administration endpoints. The script depends
# on the requests library, which you can install with `pip install
# requests`.
# 
# The admin account in the script is active -- feel free to
# retire/activate some subjects to make sure it's working for you.
# 
# Since this is all new code, there may still be some rough edges to work
# through, so please let me know if you have questions/problems.
# 
# -Michael
# 
# 
# 
# Administration:
# 
# Administration requests are authenticated with a combination of a
# password, a private key, a public key, and a sequence identifier.
# 
# 
# Administrators:
# 
# Accounts are created by request. A password, and private key will be
# supplied.
# 
# 
# Session:
# 
# A session begins by logging in with a POST request to /admin/login with
# { name: 'your admin name', password: 'your admin password' }
# 
# A successful login responds with a new public_key.
# 
# 
# Requests:
# 
# After establishing a session, requests can be made by signing each
# request with the name of the administrator and request key.
# 
# Request keys are generated by combining the public and private keys as
# well as a sequence identifier.
# 
# At the beginning of a session, the sequence identifier is 1. Each
# request increments this number.
# 
# In the case of an unauthorized request, sequence identifiers will no
# longer match causing the session to terminate.
# 
# If a session terminates, a new login request must be sent to restart the
# session with a new public key.
# 
# 
# Login Limits:
# 
# More than 10 unsuccessful login attempts within an hour will temporarily
# block logins by the administrator account. More than 100 unsuccessful
# login attempts within an hour will disable the administrator account. A
# developer will have to reactivate it.
# 
# Requests:
# 
# Administration requests are rate limited to less than 1,000 per hour.
# All requests, successful or not, are logged for security purposes.
# 
# Example code:
# 
# Subject Administration
# 
# Method  Action      Path                                                Params
# PUT     activate    /admin/projects/:project_id/subjects/:id/activate   { }
# PUT     retire      /admin/projects/:project_id/subjects/:id/retire     { }
# PUT     pause       /admin/projects/:project_id/subjects/:id/pause      { }
# PUT     resume      /admin/projects/:project_id/subjects/:id/resume     { }


# OK, got it! Wrote SWITCH.py to read in a list of subject IDs, and 
# then make put requests to retire them. 

# First run SWAP with low rejection threshold to get a bunch of
# retirements to do:

SWAP.py CFHTLS-SWITCH-test.config 

# SWAP: interpreting classifications...
# SWAP: 
# SWAP: total no. of classifications processed:  37
# SWAP: saving newly retired subject IDs...
# SWAP: 19 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt

# Good - now SWITCH them:

set retirees = /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt

# SWITCH: retiring subjects listed in  /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt
# SWITCH: looks like we have 19  subjects to retire
# SWITCH: doing a dry run
# result = client.put('/projects/spacewarp/subjects/ASW000075q/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000arx/retire')
# result = client.put('/projects/spacewarp/subjects/ASW00005js/retire')
# result = client.put('/projects/spacewarp/subjects/ASW000082p/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000hlt/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000by9/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000a4m/retire')
# result = client.put('/projects/spacewarp/subjects/ASW00008wq/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000wlr/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000c6t/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000w51/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000kx4/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000s3v/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000l4y/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000d6r/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000b6n/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000tc7/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000hs9/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000kig/retire')

# OK, looks good - give it a try!

SWITCH.py $retirees

# Whoah - success?
# 
# SWITCH: retiring subjects listed in  /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt
# SWITCH: looks like we have 19  subjects to retire
# 
# ...
# 
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(0)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(1)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(2)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(3)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(4)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(5)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(6)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(7)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(8)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(9)
# ASW0000kig False
# ======================================================================

# Check with Michael!! :-)

# Yep, all good after changing response.json to response.json()

# ======================================================================
# 2013-05-08 (Wednesday) 14:29 BST

# Launch day! :-)
# First batch of classifications from Michael. First tidy away all the
# beta work... and also the SWITCH test:

mkdir -p Beta
mv *beta* Beta/
mv spacewarps-2013-04-06_20-07-28* Beta/

mkdir -p SWITCH-test
mv *-SWITCH* SWITCH-test/
mv spacewarp_* SWITCH-test/
mv CFHTLS* SWITCH-test/

# Now make a working directory:
mkdir -p 2013-05-08
cd 2013-05-08

# OK, get new db:

set url = "https://zooniverse-code.s3.amazonaws.com/databases/2013-05-08/ouroboros_projects/spacewarp_2013-05-08.tar.gz?AWSAccessKeyId=AKIAJHHZ7KLFECQKTS7A&Expires=1368612013&Signature=QRK%2BOV37H6khTqgqoTu0z%2FLO%2F2s%3D"

set dbfile = `echo "$url" | cut -d '?' -f1 | cut -d'/' -f7`
set logfile = ${dbfile:r:r}.wget.log
wget -O $dbfile "$url" >& $logfile

# OK, SWIPE this:

SWIPE.csh $dbfile

# Good! This script should write a config file.

SWAP.py startup.config

# Nice - 10,000 classifications of 8000 subjects by 300 people. Crowd
# look very similar to beta! Result :-)

# ======================================================================
# 2013-05-09 (Thursday) 09:46 BST

# Gosh - half a million classifications in the first day!
# Better get this code working at scale - and fast.

# OK, SWAPSHOP is operational and checked in.

# Run on 1st 300,000 classifications!
# Just did:

SWAPSHOP.csh

# Got 117 candidates and 131 false negatives from first 300,000 
# classifications, in 2013-05-09

# Made plot of just false negatives for AV - quite some uncertainty,
# subjects move back and forth quite a lot. Agents are overconfident!


# Try shifting thresholds to 0.4 and 1e-7 (symmetrical). How many
# retirements? Completeness stats?

# Edited swap/startup.config, then ran:

time SWAPSHOP.csh -s CFHTLS-strict -f --fast

# --fast is no animation, so no plots until the end!

# Looks like its about 20s per batch, increasing with time though. 
# 11 batches total, plus plots, takes 8.5 mins.

# STill getting 17% false negatives, and with only 18000 retirements!


# Try a cheap version of Surhud's idea - ignore the first few clicks?
# Coded as ignore if agent.NT <= a_few_at_the_start where this 
# refers to a few training images. Lets try an extreme version with
# a_few_at_the_start = 10!

time SWAPSHOP.csh -s CFHTLS-reallystrict -f --fast

# Same number of classifiers/agents, but now many will stay at 0.5
# because they never leave... 10 training images requires about 30
# images total in the standard stream. The agent history only gets
# updated for classifications that count

# OK, I get 11500 retirements, and down to 13% false negatives
# The total number of classifications is now 267000, so we lose about
# 100k to the traiing programme! Candidates are still at 96, as with
# strict routine.

# Look at the images! Well, ones that are still negative:

mkdir training_false_negatives
set repo = ../2013-05-09/training_false_negatives

foreach url ( `cat CFHTLS-reallystrict_2013-05-09_01:38:24/CFHTLS-reallystrict_2013-05-09_01:38:24_training_false_negatives.txt` )
    set png = $url:t
    cp $repo/$png training_false_negatives/.
end

# OK, we had all but 3 already.
# cp: ../2013-05-09/training_false_negatives/5183f151e4bb2102190044e2.png: No such file or directory
# cp: ../2013-05-09/training_false_negatives/5183f151e4bb210219007187.png: No such file or directory
# cp: ../2013-05-09/training_false_negatives/5183f151e4bb210219004b03.png: No such file or directory

# What do *these* look like?

# Mostly LRGs with rings hidden in depths. All fairly tricky.
# ======================================================================
# 2013-05-10 (Friday) 12:17 BST

# New db! Try analysing with the really straict settings that are
# currently checked in:

SWIPE.csh spacewarp_2013-05-10.tar.gz

time SWAPSHOP.csh -s CFHTLS-reallystrict -f --fast

# Hmm - weird. 1.08 million classifications, but report only shows 
# 396281! Still, 15% lenses missed, 82% completeness, 99.8% purity
# in sims vs duds. 33078 subjects to retire. Leave the candidates in
# to be inspected more often! FPs are not a problem.

# Try running without the 10-subject training period.

time SWAPSHOP.csh -s CFHTLS-strict -f --fast

# OK, hmm. Seems I am counting wrong!
# Yep - switched to agents counting all classifications, 
# but subjects only counts that matter to them.

# OK, last tests: try really strict with 95% detection threshold, 
# and retirement only at low probability end.

time SWAPSHOP.csh -s CFHTLS-reallystrict95 -f --fast -d

# Affects number of detections, but not enormously they are all highly
# classified at this point


# Talked to Amit about using simFound annotation for sims!
# Implemented, lets compare with strict (reset thresholds):

time SWAPSHOP.csh -s CFHTLS-strict-simFound -f --fast

# Hmm - looks very odd. Save and investigate...
# Put in a print statement in mongodb.digest - what's going on?

time SWAPSHOP.csh -s CFHTLS-strict-simFound -f --fast -t 1

# OK, ran with verbose and one_by_one. Results seem plausible,
# but there aren't too many hits on the sims!

# In db.digest: kind,N_markers,simFound,result,truth =  sim 0 false NOT LENS
# SWAP: --------------------------------------------------------------------------
# SWAP: Subject 5183f151e4bb210219005ac5 was classified by 63.250.229.206
# SWAP: he/she said NOT when it was actually LENS
# SWAP: their agent reckons their contribution (in bits) =  1.99471942959
# SWAP: while estimating their PL,PD as  0.152173913043 0.933333333333
# SWAP: and the subject's new probability as  0.000159955385968

# NB information is wrong...
# If accuracy is missing, then we get a bunch of people just saying 
# no to everything! And then subjects just fall straight. 


# Try initialNL = 5 or so to allow for mistakes early on, and compare
# use_marker_positions = True and False, on the first 300k.

time SWAPSHOP.csh -s CFHTLS-strict-NO-useXY -f --fast -t 6

# and then: 

time SWAPSHOP.csh -s CFHTLS-strict-YES-useXY -f --fast -t 6

# OK! Compare numbers:

# Very similar, but:
# 
# Quantity        NO     YES
# Retirements    3000    150 
# Missed lenses   13%     9%

# ie the bureau is more circumspect. And much slower to retire!

# DECISION: use XY, but investigate skepticism. These results used
# skepticism = 3! Also, need more data. Run on all of 2010-05-10.

# Now, vary agent skepticism (initialNL,ND = 2 + skepticism).
# Also, bump up detection threshold to 0.95!

time SWAPSHOP.csh -s CFHTLS-skeptic00 -f --fast

time SWAPSHOP.csh -s CFHTLS-skeptic03 -f --fast

time SWAPSHOP.csh -s CFHTLS-skeptic08 -f --fast

# Skepticism              0        3        8 
# Retirements         28489    25574    21271
# Age at retirement    15.6     17.8     20.3
# Missed lenses        18.5%    18.3%    18.1%
# Candidates            148       64       21

# Looks like skepticism slows things down without affecting false -ve 
# rate very much. I think we should go with skepticism = 0.
 
# Using 15 classifications per retirement is expensive.
# 400,000 * 15 = 6 million classifications!
# Maybe not that many: 375 each from 16,000 people

# Trajectories are roughly horizontal by the time we get close to 
# threshold - good, there is an end in sight! 


# What if using marker positions is causing a high false negative rate
# somehow? By giving people low PL values, so that they are disbelieved?
# Try skeptic00 with marker positions turned off.
 
time SWAPSHOP.csh -s CFHTLS-skeptic00-noXY -f --fast

# BTW each full run (10^6 classifications) takes about 14 mins.

# Use XY positions?     Yes       No
# Retirements         28489    35835
# Age at retirement    15.6     10.7
# Missed lenses        18.5%    20.1%
# Candidates            148      159

# So XY gives a *better* false neg rate! Good. 

# So, what about a training period? Level 1 has 20 subjects, 1 in 5 sims
# and 1 in 5 duds, so 8 training images total. Try ignoring all of level
# 1 and see what we get compared to the standard setup.

time SWAPSHOP.csh -s CFHTLS-skeptic00-ignore8 -f --fast -t 20

# (Note that I SWIPED the new db in the meantime - oops)

# Run:                skeptic00      ignore8
# Classifications        666864       536099
# Retirements             28489        27346
# Age at retirement        15.6         12.8
# Missed lenses            18.5%        16.3%
# Candidates                148          146

# So its cleaner and cheaper. I think we have to do this. Make it 
# past level 2 and your classifications start to count! Rough
# qualification is 20 classifications - we asked for 40 in the PR. 
# We can always go back later and make more use of the ignored clicks,
# with a better model for how the agents learn. 

# OK, done! Retirement plan is skeptic00, defined by this config:
# 
# skepticism: 0
#
# a_few_at_the_start: 8
# 
# use_marker_positions: True
# 
# detection_threshold: 0.95
# 
# rejection_threshold: 1e-7
# 
# Checked in as startup.config! Done.

# ======================================================================
# 2013-05-11 (Saturday) 19:52 BST

# Restore latest db to start production run. Run SWAPSHOP in production 
# directory so that we can re-use the pickles.

mkdir -p $SWAP_DIR/analysis/production
chdir $SWAP_DIR/analysis/production

SWIPE.csh spacewarp_2013-05-11.gz

# OK, now SWAPSHOP with simple survey name. No need for -f once we are 
# rolling!

SWAPSHOP.csh -s CFHTLS -f --fast

# OK good - we are rolling!

# ======================================================================
# 2013-05-12 (Sunday) 19:57 CEST

# Right, more retirements! 

SWIPE.csh spacewarp_2013-05-12.gz

SWAPSHOP.csh -s CFHTLS --fast

# Had to do retirments by hand, as script got it wrong to start with. 
# Here's how I recovered:

wc -l */*retire*
#    33648 CFHTLS_2013-05-11_10:04:13/CFHTLS_2013-05-11_10:04:13_retire_these.txt
#    36706 CFHTLS_2013-05-12_10:05:01/CFHTLS_2013-05-12_10:05:01_retire_these.txt

set survey = CFHTLS
\set latest = `\ls -dtr ${survey}_????-??-??_??:??:?? | tail -1`

set previousretirees = CFHTLS_previously_retired.txt
cat $previousretirees         | sort > old
cat $latest/*retire_these.txt | sort > new
sdiff -s old new | & cut -d'>' -f2 > diff
wc -l old new diff
#    33648 old
#    36706 new
#     3058 diff
mv diff CFHTLS_production_retire_these.txt

# Good! Retire these:

SWITCH.py CFHTLS_production_retire_these.txt

# ================================================================================
#                    SWITCH: the Space Warps Retirement Plan                      
# ================================================================================
# SWITCH: retiring subjects listed in  CFHTLS_production_retire_these.txt
# SWITCH: looks like we have 3058  subjects to retire
# SWITCH: successfully retired subject ASW0000008
# SWITCH: successfully retired subject ASW000000t
# SWITCH: successfully retired subject ASW0000016
# SWITCH: successfully retired subject ASW000001h
# SWITCH: successfully retired subject ASW000001i
# SWITCH: successfully retired subject ASW000001k
# SWITCH: successfully retired subject ASW000001o

# etc . Success! 

# Well, partially. Got back fom dinner to find:

# SWITCH: retirement fail:  ASW0000chb False
# failed: unauthorized, check your credentials(0)

# for a whole bunch of subjects. Emailed Parrish for advice.

# ======================================================================
# 2013-05-14 (Tuesday) 00:54 CEST

# OK, I hit the retirements per hour limit. Michael put it up to 5000
# That means we can do 1 retirment every 0.72 secs. Add sleep 0.7 to 
# code, and run overnight! We probably only did 3000 yesterday before it
# got locked out, so might as well start from teh beginning gain.

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Great, now they are all done!

# ======================================================================
# 2013-05-15 (Wednesday) 00:38 CEST

# Catching up - need to do both 13 and 14 May dumps.

SWIPE.csh spacewarp_2013-05-13.gz
SWAPSHOP.csh -s CFHTLS --fast

# Got 7463 subjects to retire - do in 3 batches:

head -2500 CFHTLS_production_retire_these.txt > batch1.txt
head -5000 CFHTLS_production_retire_these.txt | tail -2500 > batch2.txt
tail -2463 CFHTLS_production_retire_these.txt > batch3.txt

SWITCH.py batch1.txt > retirement1.log &

# Do the following next!! 2013-05-15 (Wednesday) 09:36 CEST

SWITCH.py batch2.txt > retirement2.log &
SWITCH.py batch3.txt > retirement3.log &

# Phew - just finished. That was an effort. 2013-05-17 (Friday) 01:04 CEST

# Tomorrow, need to process 2013-05-14, 15, 16... Starting with SWIPE.

# ======================================================================
# 2013-05-17 (Friday) 18:51 CEST

# On plane home from Copenhagen. Catch up!
# Need to deal with interrupted retirements in SWITCH.

# First, run SWAPSHOP on latest db! Make plots later...

SWIPE.csh spacewarp_2013-05-17.gz
SWAPSHOP.csh -s CFHTLS --fast

# SWAP: report compiled as /Users/pjm/public_html/SpaceWarps/Science/analysis/production/CFHTLS_2013-05-17_10:07:53/CFHTLS_2013-05-17_10:07:53_report.pdf
# ================================================================================
# SWAPSHOP: if you want, you can go ahead and retire 40595 subjects with
#  
#           SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Report shows 84130 subjects, 63722 to be retired. 
# Numbers don't add up! Check lists:

sort -n CFHTLS_previously_retired.txt | uniq | sed s/' '//g | grep 'ASW' > old
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new

tabs2spaces -n 0 old new

wc -l CFHTLS_previously_retired.txt old CFHTLS_production_retire_these.txt new
#    44169 CFHTLS_previously_retired.txt
#    44169 old
#    40595 CFHTLS_production_retire_these.txt
#    33132 new

# Retirement list has some repeats in it!
# Also, sdiff does not work for finding difference between files.
# Brute force it! First, update these files.

mv new CFHTLS_production_retire_these.txt
rm old

\rm junk
foreach ID ( `cat CFHTLS_production_retire_these.txt` )
  set done = `grep $ID CFHTLS_previously_retired.txt | wc -l`
  if (! $done) echo $ID >> junk
end  
wc -l junk
#   25669 junk   

# OK, 44169 old + 25669 new = 69838
# Why is this different from the 63722 listed?
# Did many of the old ones not actually get retired?
# Safe thing to do is retire eveything in 
# CFHTLS_production_retire_these.txt - won't make any difference to 
# re-retire things, it just takes longer. Run overnight!

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &


# ======================================================================
# 2013-05-19 (Sunday) 09:35 BST

# Run SWAPSHOP on latest db. Hopefully retirements are now cleaned up 
# following yesterday's run:

SWIPE.csh spacewarp_2013-05-18.gz
SWAPSHOP.csh -s CFHTLS --fast

# SWAPSHOP: if you want, you can go ahead and retire 15228 subjects with
#  
#           SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &
 
# Hmm - still not counting right?

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#     7765 new

# Hmm. Need to fix this. Over-write for now!

mv new CFHTLS_production_retire_these.txt

# Pausing retirment plan while we wait for Michael...

# 2013-05-20 (Monday) 23:03 BST
# OK got green light!

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# OK good! Onto May 20.
# 2013-05-21 (Tuesday) 10:54 BST

SWIPE.csh spacewarp_2013-05-21.gz
SWAPSHOP.csh -s CFHTLS --fast

# Wow - 32517 retirements?!

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new

# OK, "only" 17289 really...

mv new CFHTLS_production_retire_these.txt

# Retire them!

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-22 (Wednesday) 13:24 BST

SWIPE.csh spacewarp_2013-05-22.gz
SWAPSHOP.csh -s CFHTLS --fast
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   19193 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-23 (Thursday) 12:45 BST

SWIPE.csh spacewarp_2013-05-23.gz
SWAPSHOP.csh -s CFHTLS --fast
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   23533 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-24 (Friday) 12:17 BST

SWIPE.csh spacewarp_2013-05-24.gz
SWAPSHOP.csh -s CFHTLS --fast
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   
mv new CFHTLS_production_retire_these.txt

# Try retiring from SLAC: Fail! No requests module. Ho hum.

nohup SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-26 (Sunday) 21:00 BST

SWIPE.csh spacewarp_2013-05-26.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   44392 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-27 (Monday) 17:44 BST

SWIPE.csh spacewarp_2013-05-27.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   49761 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Wow - that won't leave many...

# ======================================================================
# 2013-05-26 (Sunday) 21:00 BST

SWIPE.csh spacewarp_2013-05-28.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#    54393 new  
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Need to get D6 in? Wait for a bit.

# ======================================================================
# 2013-05-30 (Thursday) 09:59 BST

SWIPE.csh spacewarp_2013-05-29.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#  58484 new  
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Retire these duds!

# Ran out of time - had to abort:
wc -l retirement.log
#   28238 retirement.log

# Restart on Friday night from New York!

# ======================================================================
# 2013-05-31 (Friday) 23:50 EDT

SWIPE.csh spacewarp_2013-05-31.gz
SWAPSHOP.csh -s CFHTLS --fast

# SWAP: interpreting up to 50000  classifications...
# ERROR: AttributeError: 'NoneType' object has no attribute 'has_key' [swap.mongodb]
# Traceback (most recent call last):
#   File "/Users/pjm/public_html/SpaceWarps/Science/analysis/SWAP.py", line 422, in <module>
#     SWAP(sys.argv[1:])
#   File "/Users/pjm/public_html/SpaceWarps/Science/analysis/SWAP.py", line 203, in SWAP
#     items = db.digest(classification,method=use_marker_positions)
#   File "/Users/pjm/public_html/SpaceWarps/Science/analysis/swap/mongodb.py", line 154, in digest
#     if subject.has_key('group_id'):
# AttributeError: 'NoneType' object has no attribute 'has_key'
# SWAPSHOP: if you want, you can go ahead and retire 140567 subjects with
#  
#           SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Hmm - wonder what that problem is?

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#    58484 new  
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Off we go again! Run overnight.

# ======================================================================
# 2013-06-01 (Saturday) 22:18 EDT

SWIPE.csh spacewarp_2013-06-01.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   73358 new 
mv new CFHTLS_production_retire_these.txt

# Seems very high! Are retirement requests not working?
# Compare log with new list:

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored"
echo "List of subjects to be retired afresh:"
wc -l new

# 58484 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#   14874 new

mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-03 (Monday) 08:16 EDT

SWIPE.csh spacewarp_2013-06-03.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   65263 new 
mv new CFHTLS_production_retire_these.txt

# Seems very high! Are retirement requests not working?
# Compare log with new list:

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored"
echo "List of subjects to be retired afresh:"
wc -l new

# 0 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
# wc -l new
#    65263 new

# Wow. 
# OK, better get started! :-)

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Stop part way through:

grep -n ASW0001jfc CFHTLS_production_retire_these.txt 
# 12195:
tail -n +12196 CFHTLS_production_retire_these.txt > \
               CFHTLS_production_now_retire_these.txt

SWITCH.py CFHTLS_production_now_retire_these.txt >> retirement.log &

# And again!

grep -n ASW000261b CFHTLS_production_retire_these.txt 
# 30106:
tail -n +30107 CFHTLS_production_retire_these.txt > \
               CFHTLS_production_and_now_retire_these.txt

SWITCH.py CFHTLS_production_and_now_retire_these.txt >> retirement.log &

# And again...

grep -n ASW0002lsr CFHTLS_production_retire_these.txt 
# 45799:
tail -n +45800 CFHTLS_production_retire_these.txt > \
               CFHTLS_production_and_NOW_retire_these.txt

SWITCH.py CFHTLS_production_and_NOW_retire_these.txt >> retirement.log &


# ======================================================================
# 2013-06-05 (Wednesday) 07:45 EDT

SWIPE.csh spacewarp_2013-06-05.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#  72222 new
mv new CFHTLS_production_retire_these.txt

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored"
echo "List of subjects to be retired afresh:"
wc -l new

# 65260 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#     6959 new

mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Got a bunch of these:
# 
# HTTPSConnectionPool(host='api.zooniverse.org', port=443): Max retries exceeded with url: /admin/login (Caused by <class 'socket.error'>: [Errno 50] Network is down)(1)

# Killed process. Restart:

grep ASW retirement.log | grep -v fail | grep -v error | tail -1
# SWITCH: successfully retired subject ASW0004weo
grep -n ASW0004weo CFHTLS_production_retire_these.txt 
# 5904:ASW0004weo
tail -n +5905 CFHTLS_production_retire_these.txt > \
              CFHTLS_production_now_retire_these.txt

SWITCH.py CFHTLS_production_now_retire_these.txt >> retirement.log &

# ======================================================================
# 2013-06-06 (Thursday) 23:01 CDT

SWIPE.csh spacewarp_2013-06-06.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 70950  Pretty sure these are not all new but retire them anyway...
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Killed job in order to start fresh on Sunday!

grep uccess retirement.log | wc -l
#    58595
# ======================================================================
# 2013-06-09 (Sunday) 09:46 PDT

SWIPE.csh spacewarp_2013-06-09.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 84664 new
mv new CFHTLS_production_retire_these.txt

# OK, these have to be repeats. Compare with yesterday's retiremnets.

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | grep uccess | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new

# 58593 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#   26071 new

# OK, that's plausible! Off we go then:
mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# All done!

# ======================================================================
# 2013-06-10 (Monday) 08:07 PDT

SWIPE.csh spacewarp_2013-06-10.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 64721 new
mv new CFHTLS_production_retire_these.txt

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | grep uccess | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new

# 3058 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#    61663 new

mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# 66072 retirement.log

# ======================================================================
# 2013-06-12 (Wednesday) 14:43 PDT

SWIPE.csh spacewarp_2013-06-12.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 10390 new
mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-13 (Thursday) 08:35 PDT

# Today we compare simple and fuzzy trajectories! First do simple run,
# as control:

SWIPE.csh spacewarp_2013-06-13.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 13908 new
mv new CFHTLS_production_retire_these.txt

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | grep uccess | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new

# 10390 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#     3518 new
mv new CFHTLS_production_retire_these.txt

# OK, don't retire these - let's now compare with the fuzzies! 
# First need to accept Surhud's pull request, then check out the code
# into the Space Warps directory. If I decide to go back, I can just
# check out the previous version of SWAP... but Ill need a separate 
# workspace, at the same level as "production". Call it: fuzzy.

# Need to get sartup.config right - check Surhud's emails.

# Here's the pull output:

# From github.com:drphilmarshall/SpaceWarps
#    d43ef4b..d77816a  master     -> origin/master
# Updating d43ef4b..d77816a
# Fast-forward
#  analysis/SWAP.py                |    6 ++
#  analysis/SWAPSHOP.csh           |    3 +
#  analysis/swap/agent.py          |   30 ++++++++
#  analysis/swap/bureau.py         |    8 +++
#  analysis/swap/collection.py     |   31 +++++---
#  analysis/swap/config.py         |    1 +
#  analysis/swap/logging.py        |   18 +++--
#  analysis/swap/production.config |    8 +--
#  analysis/swap/startup.config    |    6 +-
#  analysis/swap/subject.py        |   65 ++++++++++++-----
#  doc/sw-system.tex               |  151 ++++++++++++++++++---------------------
#  11 files changed, 204 insertions(+), 123 deletions(-)

# Forgot to tag it before pulling, oops. Ah well! Onwards :-)

SWAPSHOP.csh -s CFHTLS --fast --startup

# startup option brings over startup.config etc. Ntrajectory is set in 
# the header of subject.py, which is a bug... But its set to 50
# as recommenedded by Surhud.

# Timing runs: each iteration of 50,000 classifications takes about a
# minute, a little over. With 5 million classifications, the whole 
# SWAPSHOP run will take at least 100 mins - say 2 hours.

# Then, compare reports, and also lists of retirees. This needs to be
# done carefully, using grep and awk etc, not sdiff, probably. 

# had to pause after 139 iterations - and it didn not restart cleanly
# :-(

SWAPSHOP.csh -s CFHTLS --fast

# OK, compare reports and retirements. Not easy, as we have to
# accumulate them uniquely. First clean up the fuzzy retirements!

sort -n CFHTLS_fuzzy_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 176818 new  GOOD!
mv new CFHTLS_fuzzy_retire_these.txt

mkdir -p comparison
cp CFHTLS_fuzzy_retire_these.txt CFHTLS_fuzzy_report.pdf comparison/

# OK, now bring in all retirements done so far from production...

cp ../production/CFHTLS_previously_retired.txt \
        comparison/CFHTLS_production_previously_retired.txt
cp ../production/CFHTLS_production_retire_these.txt comparison/

# Hmm - previous retirements will have doubles in them from repeated 
# days efforts:

cd comparison

cat CFHTLS_production_previously_retired.txt | awk '{print $1}' | \
  grep 'ASW' | sort -n | uniq > new
wc -l CFHTLS_production_previously_retired.txt new
#   275170 CFHTLS_production_previously_retired.txt
#   177563 new
# OK, potentially very similar! Let's see.
mv new CFHTLS_production_previously_retired.txt

# Just to make sure, do same thing to the fuzzies:
cat CFHTLS_fuzzy_retire_these.txt | awk '{print $1}' | \
  grep 'ASW' | sort -n | uniq > new
wc -l CFHTLS_fuzzy_retire_these.txt new
#   176818 CFHTLS_fuzzy_retire_these.txt
#   176818 new
mv new CFHTLS_fuzzy_retire_these.txt

# Good. Now, we need two lists: 
# 1) Subjects to retire now
# 2) Subjects to resurrect now

# 1) Take the fuzzy retirement list, and remove all that appear in
#    either the production previous retirements, or the production
#    retire these list.

cat CFHTLS_production_previously_retired.txt \
    CFHTLS_production_retire_these.txt | \
  awk '{print $1}' | grep 'ASW' | sort -n | uniq \
  > CFHTLS_production_all_retirements.txt
  
set retirements = CFHTLS_to_turn_fuzzy_retire_these.txt
set count = 0
\rm -f $retirements
foreach subject ( `cat CFHTLS_fuzzy_retire_these.txt` )
   set k = `grep $subject CFHTLS_production_all_retirements.txt | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> $retirements
   endif
end
echo "$count subjects already happily retired, can be ignored" ; \
echo "List of subjects to be retired:" ; \
wc -l $retirements

# 176463 subjects already happily retired, can be ignored
# List of subjects to be retired:
#      355 CFHTLS_to_turn_fuzzy_retire_these.txt

# Cool!


# 2) Take the production previously retired list, and remove the 
#    subjects that appear in the fuzzy retirement list.

set resurrections = CFHTLS_to_turn_fuzzy_resurrect_these.txt
set count = 0
\rm -f $resurrections
foreach subject ( `cat CFHTLS_production_previously_retired.txt` )
   set k = `grep $subject CFHTLS_fuzzy_retire_these.txt | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> $resurrections
   endif
end
echo "$count subjects already happily retired, can be ignored" ; \
echo "List of subjects to be resurrected:" ; \
wc -l $resurrections

# 173060 subjects already happily retired, can be ignored
# List of subjects to be resurrected:
#     4503 CFHTLS_to_turn_fuzzy_resurrect_these.txt

# OK, execute this, then continue running SWITCH in fuzzy directory
# tomorrow? Yes - state is read from pickles, not from mongodb.

SWITCH.py CFHTLS_to_turn_fuzzy_retire_these.txt > turning_fuzzy_retirement.log &

SWITCH.py -r CFHTLS_to_turn_fuzzy_resurrect_these.txt > turning_fuzzy_resurrection.log &

# OK, done! Now, to start SWAPPING from directory fuzzy.

# ======================================================================
# 2013-06-15 (Saturday) 09:44 PDT

cd fuzzy

SWIPE.csh spacewarp_2013-06-15.gz
SWAPSHOP.csh -s CFHTLS --fast

# Had to restart after trashing production mongo directory and re-SWIPEing.

# SWAPSHOP: if you want, you can go ahead and retire 184711 subjects with
#  
#           SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# Hmm. Seems like a lot! As if it didn't know what had been retired...
# Need to compare with log somehow.

sort -n CFHTLS_fuzzy_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 184711 new
mv new CFHTLS_fuzzy_retire_these.txt

# OK, try faking a retirement log!

cat comparison/CFHTLS_fuzzy_retire_these.txt > retirement.txt

# Now do the line by line comparison:

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_fuzzy_retire_these.txt` )
   set k = `grep $subject retirement.txt | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new
# 176818 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#     7893 new

# Over-write with new file:
mv new CFHTLS_fuzzy_retire_these.txt

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# OK done - we are up and running.

# ======================================================================


